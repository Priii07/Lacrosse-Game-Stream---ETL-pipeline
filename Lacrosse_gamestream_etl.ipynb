{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40da595c",
      "metadata": {},
      "source": [
        "## <font color='black'> Spark Connection and Intial Setup </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9ad58a-dd5b-4fb5-b31f-253fdd8aa690",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "user = username\n",
        "passwd = passwd\n",
        "s3_bucket = bucket\n",
        "s3_server = s3_server_url\n",
        "s3_access_key = accesskey\n",
        "s3_secret_key = secretkey\n",
        "mongo_uri = f\"mongodb://{user}:{passwd}@mongo:27017/admin?authSource=admin\"\n",
        "server_name = \"jdbc:sqlserver://mssql\"\n",
        "database_name = dbname\n",
        "mssql_user = sql_user\n",
        "mssql_pw = sql_passwd\n",
        "mssql_url = (\n",
        "    server_name\n",
        "    + \";\"\n",
        "    + \"databaseName=\"\n",
        "    + database_name\n",
        "    + \";encrypt=true;trustServerCertificate=true;\"\n",
        ")\n",
        "\n",
        "jars = [\n",
        "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
        "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
        "    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n",
        "    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\",\n",
        "]\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.master(\"local\")\n",
        "    .appName(\"jupyter-pyspark\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(jars))\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_server_url)\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", accesskey)\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", secretkey)\n",
        "    .config(\"spark.hadoop.fs.s3a.fast.upload\", True)\n",
        "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "    .config(\"spark.mongodb.input.uri\", mongo_uri)\n",
        "    .config(\"spark.mongodb.output.uri\", mongo_uri)\n",
        "    .getOrCreate()\n",
        ")\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a238996e",
      "metadata": {},
      "source": [
        "## <font color='black'>Importing necessary pyspark libraries</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a843ea-c445-4510-b89e-64ddf173e17c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, col, expr, sum, row_number, struct, lit, when, collect_list\n",
        "from pyspark.sql.column import Column\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, LongType,ArrayType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ca471b",
      "metadata": {},
      "source": [
        "## <font color='black'> Reading players data from sql server. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c0ae8c-0e0f-41df-9d2c-e814ff0ac077",
      "metadata": {},
      "outputs": [],
      "source": [
        "players = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", tablename) \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1c42ac",
      "metadata": {},
      "source": [
        "## <font color='black'> Reading teams data from sql server. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2632e6-0227-4ca2-a2ae-9db3fa0287ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "teams = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", tablename2) \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f5ef12",
      "metadata": {},
      "source": [
        "## <font color='black'> Q1) Write a drill SQL query to list the team and player data. Specifically display team name, team wins, team losses player name, player shots and player goals.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a292102e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Listing the data in the players table and teams table\n",
        "select t.name as Team_Name, \n",
        "        t.wins as Team_wins, \n",
        "        t.losses as Team_Losses, \n",
        "        p.name as Player_Name, p.shots, p.goals \n",
        "from mssql.dbo.players p JOIN mssql.dbo.teams t ON p.teamid = t.id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b1622a",
      "metadata": {},
      "source": [
        "![Alt text](image-28.png)\n",
        "![Alt text](image-29.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb13e130",
      "metadata": {},
      "source": [
        "## <font color='black'> Q2) Write a drill SQL query to display the gamestream. Label each of the columns in the gamestream with their appropriate columns names from the data dictionary. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc993c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "SELECT columns[0]as event_id,\n",
        "       columns[1] as timestamp_of_goal,\n",
        "       columns[2] as team_id,\n",
        "       columns[3] as player_jersey_number,\n",
        "       columns[4] as goal_nogoal \n",
        "FROM minio.`gamestream.txt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5f3391",
      "metadata": {},
      "source": [
        "![Alt text](image-30.png)\n",
        "![Alt text](image-31.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a3ab3b",
      "metadata": {},
      "source": [
        "## <font color='black'> Q3) Write pyspark code (in SQL or DataFrame API) to display the gamestream. Label each of the columns in the gamestream with their appropriate columns names from the data dictionary.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a33009",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Read the gamestream.txt from minio\n",
        "url = f\"s3a://gamestreams/gamestream.txt\"\n",
        "read_text = spark.read.text(url)\n",
        "read_text.show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6fca168",
      "metadata": {},
      "source": [
        "![Alt text](image-32.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea17408",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Splitting the columns as per the data dictionary\n",
        "from pyspark.sql.functions import split, col\n",
        "games_data = read_text.withColumn(\"event_id\", split(read_text[\"value\"], \" \").getItem(0))\n",
        "games_data = games_data.withColumn(\"timestamp_of_goal\", split(read_text[\"value\"], \" \").getItem(1))\n",
        "games_data = games_data.withColumn(\"team_id\", split(read_text[\"value\"], \" \").getItem(2))\n",
        "games_data = games_data.withColumn(\"player_jersey_number\", split(read_text[\"value\"], \" \").getItem(3))\n",
        "games_data = games_data.withColumn(\"goal_nogoal\", split(read_text[\"value\"], \" \").getItem(4))\n",
        "games_data = games_data.drop(col(\"value\"))\n",
        "\n",
        "games_data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3775b58",
      "metadata": {},
      "source": [
        "![Alt text](image-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79df92a",
      "metadata": {},
      "source": [
        "## <font color='black'> Q4) Write pyspark code (in SQL or DataFrame API) to group the gamestream by team/player id adding up the shots and goals.</font>\n",
        "## <font color='black'>- Include the team score.</font>\n",
        "## <font color ='black'>- Include the latest event id and the timestamp for that event id.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8af6fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Changing to the appropriate data types\n",
        "games_data =  games_data.withColumn(\"goal_nogoal\",col(\"goal_nogoal\").cast(\"integer\"))\n",
        "games_data =  games_data.withColumn(\"team_id\",col(\"team_id\").cast(\"integer\"))\n",
        "games_data =  games_data.withColumn(\"event_id\",col(\"event_id\").cast(\"integer\"))\n",
        "games_data =  games_data.withColumn(\"player_jersey_number\",col(\"player_jersey_number\").cast(\"integer\"))\n",
        "games_data.show(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815f17ca",
      "metadata": {},
      "source": [
        "![Alt text](image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0464d9f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "games_data.createOrReplaceTempView(\"games_data_1\")\n",
        "\n",
        "stream_data = spark.sql(''' \n",
        "\n",
        "select (event_id) _id,(timestamp_of_goal) as timestamp,  player_jersey_number, team_id,  \n",
        "SUM(goal_nogoal) OVER (PARTITION BY team_id) as team_score,\n",
        "COUNT(goal_nogoal) OVER(PARTITION BY team_id, player_jersey_number) as player_shots,\n",
        "SUM(goal_nogoal) OVER(PARTITION BY team_id, player_jersey_number) as player_goals,\n",
        "max(event_id) OVER() as event_id,\n",
        "MIN(timestamp_of_goal) OVER() as max_timestamp\n",
        "from games_data_1\n",
        "ORDER BY team_id\n",
        "''')\n",
        "\n",
        "stream_data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9147e15",
      "metadata": {},
      "source": [
        "![Alt text](image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc82207c",
      "metadata": {},
      "source": [
        "## <font color='black'> Q5) Write pyspark code (in SQL or DataFrame API) to join the output from question 4 with the player and team reference data mssql so that you have the data necessary for the box score  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b74b5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Joining the players data, teams data and gamestream dataframe\n",
        "\n",
        "player = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "player = player.withColumnRenamed(\"name\",\"PlayerName\")\\\n",
        "        .withColumnRenamed(\"id\",\"PlayerID\") \\\n",
        "        .withColumnRenamed(\"number\",\"PlayerJNumber\")\n",
        "player.show()\n",
        "\n",
        "teams = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "teams = teams.withColumnRenamed(\"id\",\"id_team\")\n",
        "teams.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7500404a",
      "metadata": {},
      "source": [
        "![Alt text](image-4.png)\n",
        "\n",
        "![Alt text](image-5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1c266e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creating a final dataframe needed for the schema\n",
        "stream_player_team.createOrReplaceTempView(\"final\")\n",
        "\n",
        "final = spark.sql('''\n",
        "SELECT _id, timestamp, team_id, name, conference, wins, losses, team_score, event_id, max_timestamp, PlayerID as player_id,\n",
        "PlayerName as player_name, player_shots, player_goals \n",
        "FROM final\n",
        "''')\n",
        "\n",
        "final.show(5)\n",
        "\n",
        "print((final.count(), len(final.columns)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd22c4c",
      "metadata": {},
      "source": [
        "![Alt text](image-6.png)\n",
        "![Alt text](image-7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e5db9e",
      "metadata": {},
      "source": [
        "## <font color = 'black'> Q6) Write pyspark code (in SQL or DataFrame API) to transform the output from question 5 into the box score document structure shown in part 3.1. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0c0752",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Question 6 \n",
        "b1 = boxscore\n",
        "#Creating a custom schema \n",
        "custom_schema = b1.select(\"_id\", \"timestamp\", when(col(\"team_id\") == 101, struct(\"team_id\",\"conference\",\"wins\",\"losses\",\"team_score\", \\\n",
        "                        array(struct(\"player_id\",\"player_name\",\"player_shots\",\"player_goals\",\"pct\").alias(\"player\")).alias(\"players\")).alias(\"home\")).alias(\"Home\"), \\\n",
        "             when(col(\"team_id\") == 205, struct(\"team_id\",\"conference\",\"wins\",\"losses\",\"team_score\", \\\n",
        "                    array(struct(\"player_id\",\"player_name\",\"player_shots\",\"player_goals\",\"pct\").alias(\"player\")).alias(\"players\")).alias(\"away\")).alias(\"Away\"))\n",
        "\n",
        "custom_schema.printSchema()\n",
        "custom_schema.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb08cb8",
      "metadata": {},
      "source": [
        "![Alt text](image-8.png)\n",
        "![Alt text](image-9.png)\n",
        "![Alt text](image-10.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb29234",
      "metadata": {},
      "source": [
        "## <font color='black'> Q7) Write pyspark code (in SQL or DataFrame API) to write the box score completed in question 6 to the mongo.sidearm.boxscores collection. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9250606c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: Write the gamestream to mongodb\n",
        "#Question 7 \n",
        "#Writing data to boxscore collections\n",
        "custom_schema.write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscore\").save()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca56b72",
      "metadata": {},
      "source": [
        "![Alt text](image-11.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c967d94",
      "metadata": {},
      "source": [
        "## <font color='black'> Q8) Combine parts 4-7 into a single pyspark script that will run the entire process of creating the box score document. Make sure to run this a couple of times while the game stream is going on. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11152710",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Question 8\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, ArrayType, MapType\n",
        "from pyspark.sql.functions import struct, when,col\n",
        "\n",
        "#Reading the gamestream data\n",
        "url = f\"s3a://gamestreams/gamestream.txt\"\n",
        "read_text = spark.read.text(url)\n",
        "\n",
        "\n",
        "#Creating the boxscore document\n",
        "games_data.createOrReplaceTempView(\"games_data_1\")\n",
        "\n",
        "games_data.createOrReplaceTempView(\"games_data_1\")\n",
        "\n",
        "stream_data = spark.sql(''' \n",
        "\n",
        "select (event_id) _id,(timestamp_of_goal) as timestamp,  player_jersey_number, team_id,  \n",
        "SUM(goal_nogoal) OVER (PARTITION BY team_id) as team_score,\n",
        "COUNT(goal_nogoal) OVER(PARTITION BY team_id, player_jersey_number) as player_shots,\n",
        "SUM(goal_nogoal) OVER(PARTITION BY team_id, player_jersey_number) as player_goals,\n",
        "max(event_id) OVER() as event_id,\n",
        "MIN(timestamp_of_goal) OVER() as max_timestamp\n",
        "from games_data_1\n",
        "ORDER BY _id\n",
        "''')\n",
        "\n",
        "stream_data.show(5)\n",
        "\n",
        "stream_player = stream_data.join(player, (stream_data.team_id == player.teamid) & (stream_data.player_jersey_number == player.PlayerJNumber))\n",
        "stream_player_team = stream_player.join(teams, stream_player.team_id == teams.id_team)\n",
        "\n",
        "\n",
        "#Creating a final dataframe needed for the schema\n",
        "stream_player_team.createOrReplaceTempView(\"final\")\n",
        "\n",
        "final = spark.sql('''\n",
        "SELECT _id, timestamp, team_id, name, conference, wins, losses, team_score, event_id, max_timestamp, PlayerID as player_id,\n",
        "PlayerName as player_name, player_shots, player_goals \n",
        "FROM final ORDER BY _id\n",
        "''')\n",
        "\n",
        "final.createOrReplaceTempView(\"boxscore\")\n",
        "\n",
        "#Creating the needed columns\n",
        "boxscore = spark.sql('''SELECT *, CASE WHEN team_id = 101 THEN \"Home\" ELSE \"Away\" END as Home_Away from boxscore\n",
        "''')\n",
        "boxscore = boxscore.withColumn(\"pct\", round(boxscore.player_goals/boxscore.player_shots ,2))\n",
        "\n",
        "b1 = boxscore\n",
        "#Creating a custom schema \n",
        "custom_schema = b1.select(\"_id\", \"timestamp\", when(col(\"team_id\") == 101, struct(\"team_id\",\"conference\",\"wins\",\"losses\",\"team_score\", \\\n",
        "                        array(struct(\"player_id\",\"player_name\",\"player_shots\",\"player_goals\",\"pct\").alias(\"player\")).alias(\"players\")).alias(\"home\")).alias(\"Home\"), \\\n",
        "             when(col(\"team_id\") == 205, struct(\"team_id\",\"conference\",\"wins\",\"losses\",\"team_score\", \\\n",
        "                    array(struct(\"player_id\",\"player_name\",\"player_shots\",\"player_goals\",\"pct\").alias(\"player\")).alias(\"players\")).alias(\"away\")).alias(\"Away\"))\n",
        "\n",
        "#Writing data to boxscore collections\n",
        "custom_schema.write.format(\"mongo\").mode(\"overwrite\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscore\").save()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e44bb0",
      "metadata": {},
      "source": [
        "![Alt text](image-15.png)\n",
        "![Alt text](image-16.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6da357",
      "metadata": {},
      "source": [
        "##  <font color='black'> Q9) Write a drill SQL query to display all the box scores. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b8783d",
      "metadata": {},
      "outputs": [],
      "source": [
        "select * from mongo.sidearm.boxscore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba3c630",
      "metadata": {},
      "source": [
        "![Alt text](image-17.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8637c851",
      "metadata": {},
      "source": [
        "## <font color='black'> Q10) Write a drill SQL query to display the latest box score. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f61b2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "WITH CTE as(\n",
        "select *, max(_id) OVER() as latest_box_score from mongo.sidearm.boxscore\n",
        ")\n",
        "select * from CTE \n",
        "where _id = latest_box_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d74e46",
      "metadata": {},
      "source": [
        "![Alt text](image-18.png)\n",
        "![Alt text](image-19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47daf0c1",
      "metadata": {},
      "source": [
        "## <font color='black'> Q11) When the game is complete, write pyspark code (in SQL or DataFrame API) update the wins and losses for the teams in the teams table. Specifically, load the teams table and update it, then display the updated data frame.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85d3c32",
      "metadata": {},
      "outputs": [],
      "source": [
        "boxscore.createOrReplaceTempView(\"teams_2\")\n",
        "query3 = '''\n",
        "WITH CTE as (\n",
        "select DISTINCT team_id, team_score from teams_2\n",
        ")\n",
        "select *, rank() over(order by team_score desc) as rnk from cte\n",
        "'''\n",
        "rank_score = spark.sql(query3)\n",
        "rank_score.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb2c881",
      "metadata": {},
      "source": [
        "![Alt text](image-20.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e2e0c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Joining the teams table with the ranks table\n",
        "teams_rank = teams.join(rank_score,teams.id_team == rank_score.team_id)\n",
        "teams_rank.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5cd47c",
      "metadata": {},
      "source": [
        "![Alt text](image-21.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50c766",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Updating the teams table\n",
        "teams_rank.createOrReplaceTempView(\"updates_teams\")\n",
        "teams_2 = spark.sql('''select id_team, name, conference, CASE WHEN rnk =1 then wins+1 else wins END as wins,\n",
        "         CASE WHEN rnk =2 then losses+1 else losses END as losses from updates_teams ''')\n",
        "\n",
        "teams_2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba76439",
      "metadata": {},
      "source": [
        "![Alt text](image-22.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25440863",
      "metadata": {},
      "source": [
        "## <font color='black'> Q12) Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.teams2 table.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9070084a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Writing to MSSQL\n",
        "teams_2.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams_2\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .save()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81378111",
      "metadata": {},
      "source": [
        "![Alt text](image-23.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baec1a1c",
      "metadata": {},
      "source": [
        "## <font color='black'> Q13) When the game is complete, write pyspark code (in SQL or DataFrame API) update the shots and goals for the players in the players table. Specifically, load the players table and update it, then display the updated data frame. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aed1512",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Updating the player shots and goals\n",
        "boxscore.createOrReplaceTempView(\"player_3\")\n",
        "player.createOrReplaceTempView(\"player_2\")\n",
        "\n",
        "updating_player = spark.sql('''\n",
        "\n",
        "select distinct p.PlayerID, p.PlayerName, p.PlayerJNumber, p.shots, p.goals, p.teamid, p1.player_shots, p1.player_goals\n",
        "from player_2 p JOIN player_3 p1 on p.teamid = p1.team_id and p.PlayerJNumber = p1.player_j_number\n",
        "order by PlayerID\n",
        "'''\n",
        ")\n",
        "updating_player.show()\n",
        "updating_player.createOrReplaceTempView(\"updating_player\")\n",
        "\n",
        "player_2 = spark.sql('''\n",
        "select PlayerID, PlayerName, PlayerJNumber, (shots + player_shots) as u_shots, (goals+player_goals) as u_goals, teamid from updating_player \n",
        "''')\n",
        "player_2.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fccd64",
      "metadata": {},
      "source": [
        "![Alt text](image-24.png)\n",
        "![Alt text](image-25.png)\n",
        "![Alt text](image-26.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b60ff8",
      "metadata": {},
      "source": [
        "## <font color='black'> Q14) Write pyspark code (in SQL or DataFrame API) to write the updated in question 11 to a new mssql.sidearmdb.players table. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30206a73",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Question14\n",
        "#Writing to MSSQL\n",
        "player_2.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players2\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .save()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6156d304",
      "metadata": {},
      "source": [
        "![Alt text](image-27.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae805f9",
      "metadata": {},
      "source": [
        "## <font color='black'> Re-write drill SQL query from question 1 to use the updated players2 and teams2 tables. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a02d6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "select t.name as Team_Name, \n",
        "       t.wins as Team_wins, \n",
        "       t.losses as Team_Losses, \n",
        "       p.PlayerName as Player_Name, \n",
        "       p.u_shots as player_shots,\n",
        "       p.u_goals as player_goals \n",
        "FROM mssql.dbo.players2 p JOIN mssql.dbo.teams_2 t ON p.teamid = t.id_team\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1754af62",
      "metadata": {},
      "source": [
        "![Alt text](image-2.png)\n",
        "![Alt text](image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de707682",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
